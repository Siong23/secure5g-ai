# 5G Network Anomaly Dataset for Intrusion Detection

This README.md contains dataset of network traffic from a real 5G test lab environment, designed for developing and testing anomaly and intrusion detection systems. The dataset includes both `benign` traffic and `malicious` traffic from **flooding** , **fuzzing** and **Denial-of-service (DoS)** attacks.

---

## <br>üìã Table of Contents

-   [Overview](#-overview)
-   [Dataset Description](#-dataset-description)
-   [Data Generation Pipeline](#-data-generation-pipeline)
-   [Feature set ](#-feature-set)

---

## <br>üìù Overview


---

## <br>üìä Dataset Description

The final dataset is available in the `/dataset` directory as `5G_anomaly_dataset.csv`. It contains network flow records, where each row represents a unidirectional or bidirectional flow of packets between a source and a destination. Each flow is described by a set of features and is labeled as either **Benign** or **Malicious**.

| Category    | Description                                      |
| :---------- | :----------------------------------------------- |
| Benign      | Normal traffic simulating user activity          |
| Malicious   | Traffic from flooding, fuzzing or Dos attacks    |

---

## <br>üß™ Data Generation Pipeline

The dataset was created using five-step process, designed to transform raw packet captures into a machine learning‚Äìready datasets(.csv)

### **Step 1: Raw data capture**

Traffic was captured in a 5G test lab environment using tools like Wireshark. The raw output is in the `.pcapng` format, which includes the **GPRS Tunnelling Protocol (GTP)** layer that encapsulates user traffic between the gNodeB and the UPF. üëâ Here is the raw data: [Raw data](https://mmuedumy-my.sharepoint.com/my?id=%2Fpersonal%2F1211110323%5Fstudent%5Fmmu%5Fedu%5Fmy%2FDocuments%2FPCAP&viewid=e3af6dff%2D37bb%2D422e%2Da01f%2D80d90eebd189&ga=1)


### **Step 2: GTP Layer Removal with Tracewrangler**

To analyze the inner IP packets, the GTP encapsulation must be removed. We used **Tracewrangler** for this purpose. It strips the GTP headers and saves the inner packets to a new `.pcapng` file.

Open TraceWrangler ‚Üí Load your `.pcapng` file ‚Üí Apply **Remove GTP-U headers** in `Edit Files` task ‚Üí Run the task

![TraceWrangler Remove GTP](images/tracewrangler.png)

![TraceWrangler Remove GTP](images/tracewrangler_2.png)

Before removing GTP:

![TraceWrangler Remove GTP](images/before_gtp.png)

After removing GTP:

![After Removing GTP](images/after_gtp.png)


### **Step 3: Network Flow Generation with Argus**

The decapsulated `.pcapng` file was then processed using the **Argus (Audit Record Generation and Utilization System)** tool to convert packet-level data into network flows. Argus aggregates packets into flows based on a 5-tuple (source IP, destination IP, source port, destination port, and protocol).

```bash
# Convert pcapng to Argus flow data
argus -r file_name.pcapng -w file_name.argus
```

### Step 4: Feature Extraction (Argus)

Once the `.argus` network flow file has been generated, we extract all **available flow-level features** using the Argus client tool (`ra`). These features are directly generated by Argus and represent network statistics collected per flow.

```bash
#Extract features using ra and save to CSV
ra -r file_name.argus -seq dur mean proto stos dtos sdsb ddsb sttl dttl pkts spkts dpkts bytes sbytes dbytes offset load sload dload loss sloss dloss ploss rate srate drate state swin dwin svid dvid stcpb dtcpb tcprtt synack ackdat -c, > file_name.csv
```

<br>You can see summarized flow data before convert to .csv, For example,

```bash
ra -r file_name.argus -s saddr daddr proto sport dport bytes pkts dur bytes
```

<br>You can also can see available extractable features. See the command at line label `-s`
```bash
ra -h
```

---  

### **Step 5: Feature Computation and Labeling with Jupyter Notebook**

After Argus feature extraction, some advanced features are computed using **Jupyter Notebook**, as Argus cannot directly calculate all statistical or derived metrics (e.g., Min, Max, Sum, Hops, Mean Packets).

We use **Anaconda**, which provides Python along with data science tools such as **Jupyter Notebook**, **Pandas**, and **NumPy**.

#### üß© Install Anaconda

Download and install from the official website:

üëâ [Anaconda Distribution](https://www.anaconda.com/products/distribution)

## üìà Feature Set

## <br>üß© Step 5: Additional Feature Computation & Labeling (Jupyter Notebook /Python)

After Argus extraction, additional features can be computed or labeled externally, so we can use Python to perform it.

### üì¶ Install & Use Anaconda (Python environment)

To simplify package management and environment setup, I use **Anaconda** (or **Miniconda**) ‚Äî it bundles Python, Jupyter Notebook, and common data science libraries (e.g. `pandas`, `numpy`, `matplotlib`).  
You can install it from:

- **Anaconda Distribution**: https://www.anaconda.com/products/distribution  

<br>After installation, we can run Jupyter Notebook in `Anaconda Prompt`

```bash
jupyter notebook
```

<br>Now we can start to extract features in Argus-extracted CSV files, open a notebook, import your csv

```bash
import pandas as pd
import numpy as np

# Load the Argus CSV output
df = pd.read_csv("file_name.csv")
```

<br>Then add and compute additional features for each `.csv` files. Here are my commands in notebook:

First, I will drop unecessary rows. 

![Remove unnecessary row](images/uneccessary.png)

```bash
# Drop rows with "man" in Proto
df = df[df['Proto'] != 'man']
```


#### <br>Add features: `Label`, `Attack Type`, `Attack Tool` 

So for in my csv files, there have:
- **Label**: Malicious
- **Attack Type**: ICMPFlood/UDPFlood/SYNFlood/HTTPFlood/SlowrateDos
- **Attack Tool**: Hping3/Goldeneye/Slowloris/Torshammer 

 üßæ Example Command
```bash
#Insert colums at the last column (default)
df["Label"] = "Malicious"
df["Attack Type"] = "SYNFlood"
df["Attack Tool"] = "Hping3"
```


#### <br>Add features: - `Sum`, `Min`, `Max`

 üßæ Command
```bash
# Insert new columns and at specific column replicate with existing column content
df.insert(3, "Sum", df["Mean"])
df.insert(4, "Min", df["Mean"])
df.insert(5, "Max", df["Mean"])
```

Before insert column at specific column, we can check the column number
```bash
# Show column names with their numbers
for i, col in enumerate(df.columns):
    print(i, col)
```


#### <br>Add features: - `SrcGap`, `DstGap`

 üßæ Command
```bash
# Insert new columns at specific column and replicate with existing fixed column content
df.insert(3, "Sum", df["Mean"])
df.insert(4, "Min", df["Mean"])
df.insert(5, "Max", df["Mean"])
```

#### <br>Add features: - `sHops`, `dHops`






